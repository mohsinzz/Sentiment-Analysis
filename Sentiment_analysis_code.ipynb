{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30638eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebe39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88370d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31c5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5e15dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.input_dataframe= self.get_input_dataframe()\n",
    "        self.output_dataframe = self.get_output_dataframe()\n",
    "        \n",
    "        self.positive_words=self.get_positive_words()\n",
    "        self.negative_words=self.get_negative_words()\n",
    "        \n",
    "        self.stop_words= self.get_stop_words()\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        self.main_code = self.main_code(0)\n",
    "        \n",
    "        #returns the input data frame \n",
    "        \n",
    "    def get_input_dataframe(self):\n",
    "        \n",
    "        input_data=pd.read_excel('C:/Sentiment_analysis_project/input.xlsx')\n",
    "        return input_data\n",
    "        \n",
    "        # returns the structure of the output data frame. \n",
    "        \n",
    "    def get_output_dataframe(self):\n",
    "        \n",
    "        output_data = pd.read_excel('C:/Sentiment_analysis_project/Output_structure.xlsx')\n",
    "        return output_data\n",
    "    \n",
    "        # main code updates the output data frame based on the calculated values. It interacts with the method named 'get_all_output_features'\n",
    "    \n",
    "    def main_code(self,i):\n",
    "        \n",
    "        self.index=i\n",
    "        url=self.input_dataframe.loc[self.index,'URL']\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        output_filename = \"C:/Sentiment_analysis_project/Extracted_text/extracted_paragraphs_{}.txt\"\n",
    "\n",
    "        output_file = output_filename.format(self.index)\n",
    "\n",
    "        min_paragraph_length = 200\n",
    "\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        words=[]\n",
    "\n",
    "        with open(output_file, \"w\") as file:\n",
    "            \n",
    "            count_of_sentences = 0\n",
    "            \n",
    "            count_of_words_with_token = 0\n",
    "            \n",
    "            total_characters=0\n",
    "            \n",
    "            count_of_pronouns =0\n",
    "            \n",
    "            \n",
    "\n",
    "            for p in paragraphs:\n",
    "                \n",
    "                para_text=p.get_text()\n",
    "                \n",
    "                if 'We provide intelligence, accelerate innovation and implement technology' not in para_text and len(para_text)>min_paragraph_length: \n",
    "                    \n",
    "                    pronounRegex = re.compile(r'I|we|my|ours|us',re.I)\n",
    "                    pronouns = pronounRegex.findall(para_text)\n",
    "                    \n",
    "                    count_of_pronouns +=len(pronouns)\n",
    "                    \n",
    "                    count_of_sentences += len(sent_tokenize(para_text))\n",
    "                    \n",
    "                    count_of_words_with_token += len(word_tokenize(para_text))\n",
    "                    \n",
    "                    file.write(para_text + \"\\n\")\n",
    "\n",
    "                    paragraph_words=para_text.split(' ')\n",
    "                    \n",
    "                    for word in paragraph_words:\n",
    "    \n",
    "                        word=word.replace(\".\",\"\")\n",
    "                        word=word.replace('\"',\"\")\n",
    "                        word=word.replace(',',\"\")\n",
    "\n",
    "                        if word.lower() not in self.stop_words and len(word)>=1:\n",
    "                        \n",
    "                            total_characters+=len(word.lower())\n",
    "                            words.append(word.lower())\n",
    "        \n",
    "        \n",
    "        \n",
    "        name_of_output_features = ['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                                   'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
    "                                   'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
    "                                   'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
    "                                   'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
    "        \n",
    "        # calling the 'get_all_output_features'\n",
    "        values_of_output_features = self.get_all_output_features(words,count_of_sentences,count_of_words_with_token,total_characters,count_of_pronouns)\n",
    "        \n",
    "        # updating the desried feaatures for ith row\n",
    "        for j in range (len(name_of_output_features)):\n",
    "            \n",
    "            self.output_dataframe.loc[i,name_of_output_features[j]]=values_of_output_features[j]\n",
    "        \n",
    "        if i<len(self.input_dataframe)-1:\n",
    "            \n",
    "        #calling the main_code method recursively for every ith row of the dataframe\n",
    "            self.main_code(i+1)\n",
    "            \n",
    "        # after iterating through everyrow and updating all the values we call this method.\n",
    "        #This will return the final output data frame\n",
    "        else:\n",
    "            final_output_dataframe = self.get_final_output_dataframe()\n",
    "        \n",
    "            return final_output_dataframe\n",
    "            \n",
    "    # get the list of positive words\n",
    "    \n",
    "    def get_positive_words(self):\n",
    "        \n",
    "        self.positive_words_list=[]\n",
    "        \n",
    "        positive_words_filename = \"C:/Sentiment_analysis_project/positive-words.txt\"\n",
    "        \n",
    "        with open(positive_words_filename,\"r\",encoding='utf-8') as file:\n",
    "            \n",
    "            for line in file:\n",
    "                \n",
    "                positive_words_inline= line.split()\n",
    "                for positive_word in positive_words_inline:\n",
    "                    self.positive_words_list.append(positive_word.lower()) \n",
    "                    \n",
    "        return self.positive_words_list\n",
    "    \n",
    "    #get the list of negative words\n",
    "    \n",
    "    def get_negative_words(self):\n",
    "        \n",
    "        self.negative_words_list=[]\n",
    "        negative_words_filename = \"C:/Sentiment_analysis_project/negative-words.txt\"\n",
    "        \n",
    "        with open(negative_words_filename,\"r\", encoding='utf-8') as file:\n",
    "            \n",
    "                \n",
    "            for line in file:\n",
    "                    \n",
    "                negative_words_inline= line.split()\n",
    "                for negative_word in negative_words_inline:\n",
    "                    self.negative_words_list.append(negative_word.lower())\n",
    "                    \n",
    "        return self.negative_words_list \n",
    "    \n",
    "    #get the list of stop words\n",
    "    \n",
    "    def get_stop_words(self) :\n",
    "        \n",
    "        self.stop_words_list=[]\n",
    "        \n",
    "        combined_stop_words_filename = \"C:/Sentiment_analysis_project/Stop_words/combined_stop_words.txt\"\n",
    "        \n",
    "        with open(combined_stop_words_filename,\"r\") as file:\n",
    "            \n",
    "            for line in file:\n",
    "                \n",
    "                combined_stop_words_inline= line.split()\n",
    "                \n",
    "                for stop_word in combined_stop_words_inline:\n",
    "                    \n",
    "                    self.stop_words_list.append(stop_word.lower())\n",
    "                    \n",
    "        currency_stop_words_filename = \"C:/Sentiment_analysis_project/Stop_words/StopWords_Currencies.txt\"\n",
    "        \n",
    "        with open(currency_stop_words_filename,\"r\") as file:\n",
    "            \n",
    "            for line in file:\n",
    "                \n",
    "                currency_stop_words_inline= line.rstrip('\\n').split('|')\n",
    "                \n",
    "                for stop_word in currency_stop_words_inline:\n",
    "                    \n",
    "                    self.stop_words_list.append(stop_word.lower())\n",
    "        \n",
    "        return self.stop_words_list\n",
    "    \n",
    "    # this methods get a list of output features for ith row.It takes the input from the 'main_code' method.\n",
    "    #This method also uses the output from get_count_of_complex_and_syllables method\n",
    "    def get_all_output_features(self,word_list_of_ith_url,count_of_sentences,count_of_words,total_useful_characters,personal_pronouns):\n",
    "        \n",
    "        self.words=word_list_of_ith_url\n",
    "        \n",
    "        positive_score=0\n",
    "        negative_score=0\n",
    "        subjectivity_score=0\n",
    "        \n",
    "        for word in self.words:\n",
    "            \n",
    "            if word in self.positive_words:\n",
    "                positive_score+=1\n",
    "                \n",
    "            if word in self.negative_words:\n",
    "                negative_score+=1\n",
    "                \n",
    "        polarity_score = ((positive_score-negative_score)/((positive_score + negative_score)+0.000001)) +0.000001\n",
    "        subjectivity_score = (positive_score+negative_score)/(len(self.words)+0.000001)\n",
    "        \n",
    "        # calling the get_count_of_complex_and_syllables method\n",
    "        \n",
    "        syllabes_and_complex_count = self.get_count_of_complex_and_syllables(self.words)\n",
    "        \n",
    "        count_of_syllables = syllabes_and_complex_count[0]\n",
    "        count_of_complex_words =syllabes_and_complex_count[1]\n",
    "        \n",
    "        if count_of_words==0:\n",
    "            \n",
    "            syllable_per_word = 0\n",
    "            avg_word_length = 0\n",
    "            percentage_of_complex_words = 0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            syllable_per_word = count_of_syllables/count_of_words\n",
    "            avg_word_length = total_useful_characters//count_of_words\n",
    "            percentage_of_complex_words = (count_of_complex_words/count_of_words)*100\n",
    "        \n",
    "        \n",
    "        if count_of_sentences==0:\n",
    "            \n",
    "                avg_sentence_length    =0\n",
    "                avg_words_per_sentence =0\n",
    "        else:\n",
    "                \n",
    "            avg_sentence_length = total_useful_characters//count_of_sentences\n",
    "        \n",
    "            avg_words_per_sentence = count_of_words//count_of_sentences\n",
    "        \n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "        return [positive_score,negative_score,polarity_score,subjectivity_score,\n",
    "                avg_sentence_length,percentage_of_complex_words,fog_index,avg_words_per_sentence,\n",
    "               count_of_complex_words,count_of_words,syllable_per_word,personal_pronouns,avg_word_length]\n",
    "    \n",
    "    # this method takes the input of the word list from the get_all_output_features method\n",
    "    # and returns a list of length 2 back into that method\n",
    "    \n",
    "    def get_count_of_complex_and_syllables(self,list_of_words):\n",
    "        \n",
    "        total_count_of_syllables =0\n",
    "        count_of_complex_words =0\n",
    "        vowels ='aeoiuy'\n",
    "        \n",
    "        for word in list_of_words:\n",
    "            count_of_syllables_in_word =0\n",
    "            \n",
    "            if word[0] in vowels:\n",
    "                count_of_syllables_in_word +=1\n",
    "                \n",
    "            if len(word)<=1:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                for word_index in range(1,len(word)):\n",
    "\n",
    "                    if word[word_index] in  vowels and word[word_index - 1] not in vowels:\n",
    "\n",
    "                        count_of_syllables_in_word +=1\n",
    "                \n",
    "            if word.endswith('e'):\n",
    "                count_of_syllables_in_word -= 1\n",
    "                \n",
    "            if word.endswith('le'):\n",
    "                count_of_syllables_in_word += 1\n",
    "                \n",
    "            if count_of_syllables_in_word == 0:\n",
    "                count_of_syllables_in_word += 1\n",
    "                \n",
    "            total_count_of_syllables += count_of_syllables_in_word \n",
    "            \n",
    "            if count_of_syllables_in_word >=3:\n",
    "                count_of_complex_words +=1\n",
    "                \n",
    "        return [total_count_of_syllables,count_of_complex_words]\n",
    "                        \n",
    "    # This method is called in the end of 'main_code' method. This returns the final output data frame\n",
    "    \n",
    "    def get_final_output_dataframe(self):\n",
    "        \n",
    "        self.output_dataframe.to_excel('C:Sentiment_analysis_project/final_output_dataframe.xlsx')\n",
    "        \n",
    "        return self.output_dataframe\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8be22cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xef in position 3551: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m obj\u001b[39m=\u001b[39msentiment_analysis()\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36msentiment_analysis.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_output_dataframe()\n\u001b[0;32m      7\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_positive_words()\n\u001b[1;32m----> 8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnegative_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_negative_words()\n\u001b[0;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_stop_words()\n\u001b[0;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstemmer \u001b[39m=\u001b[39m PorterStemmer()\n",
      "Cell \u001b[1;32mIn[36], line 149\u001b[0m, in \u001b[0;36msentiment_analysis.get_negative_words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m negative_words_filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Sentiment_analysis_project/negative-words.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(negative_words_filename,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[0;32m    151\u001b[0m         negative_words_inline\u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit()\n\u001b[0;32m    152\u001b[0m         \u001b[39mfor\u001b[39;00m negative_word \u001b[39min\u001b[39;00m negative_words_inline:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[0;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xef in position 3551: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "obj=sentiment_analysis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04caf2b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m obj\u001b[39m.\u001b[39mget_final_output_dataframe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "obj.get_final_output_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229722fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
